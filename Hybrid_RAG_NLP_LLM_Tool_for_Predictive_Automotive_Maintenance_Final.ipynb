{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gb6pKtNIEhXn"
   },
   "outputs": [],
   "source": [
    "# Install required libraries (added faiss-cpu, langchain-huggingface, and ipywidgets)\n",
    "!pip install -q langchain langchain-community transformers spacy faiss-cpu streamlit sentence-transformers accelerate bitsandbytes langchain-huggingface ipywidgets\n",
    "\n",
    "# Install localtunnel via npm (not pip) â€” Optional now, since we're using widgets\n",
    "!npm install localtunnel\n",
    "\n",
    "# Download spaCy model\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Test installation (debug: check if GPU is available for other components like embeddings)\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iho6kAYQIB3i",
    "outputId": "a088c82b-d2c7-4feb-ac13-fc0345b6f00b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy test entities: []\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import spacy\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "import streamlit as st\n",
    "\n",
    "# Load spaCy NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Print test (debug: confirm spaCy loaded)\n",
    "doc = nlp(\"Test sentence for engine failure.\")\n",
    "print(\"spaCy test entities:\", [(ent.text, ent.label_) for ent in doc.ents])  # Should print empty or basic entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8g9DrDrXRtNB"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uholJmdAIB0n",
    "outputId": "bf405860-b968-4183-bb15-1baa5988b493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/parvmodi/automotive-vehicles-engine-health-dataset\n",
      "License(s): CC0-1.0\n",
      "Downloading automotive-vehicles-engine-health-dataset.zip to /content\n",
      "  0% 0.00/595k [00:00<?, ?B/s]\n",
      "100% 595k/595k [00:00<00:00, 1.06GB/s]\n",
      "Number of documents: 500\n",
      "Sample document: Log ID: 0, Engine RPM: 700.0, Lubricant Oil Pressure: 2.493591821 bar, Coolant Pressure: 3.178980794 bar, Coolant Temp: 81.6321865 C, Engine Condition: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Download and load real Kaggle dataset for automotive engine health\n",
    "!kaggle datasets download -d parvmodi/automotive-vehicles-engine-health-dataset --unzip  # Downloads engine_health_dataset.csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('engine_data.csv')  # Assumes the file name from dataset\n",
    "\n",
    "# Convert rows to text documents (simulate logs)\n",
    "documents = []\n",
    "for idx, row in df.iterrows():\n",
    "    log = f\"Log ID: {row.get('ID', idx)}, Engine RPM: {row['Engine rpm']}, Lubricant Oil Pressure: {row.get('Lub oil pressure', 'N/A')} bar, Coolant Pressure: {row.get('Coolant pressure', 'N/A')} bar, Coolant Temp: {row['Coolant temp']} C, Engine Condition: {row['Engine Condition']}\"\n",
    "    documents.append(log)\n",
    "\n",
    "# Limit to first 500 for efficiency (dataset has ~19k rows)\n",
    "documents = documents[:500]\n",
    "\n",
    "# Test print (debug: check data loaded)\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "print(\"Sample document:\", documents[0])  # Inspect one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405,
     "referenced_widgets": [
      "ee946d6e375e4b5bbd219184c0338834",
      "6139799229f740578465cdd3939a5171",
      "aa1d287cfcbf4e67958b3d59a059a914",
      "31c60c9c3dd74d3db45195e20c7a17f7",
      "b69cdbaf1c4e4103b0ab6ce815183499",
      "5652b922f7c041d28617aa70b61f4e52",
      "6ab9fb0125e44788904959abf1f43cd5",
      "b548b834967241a783ae0f59bec7cabd",
      "0f5a90f8f7b24c7aa24665e35f8c38b6",
      "3a64bc8bbfee40098a77629b778dde5f",
      "bafebc222085446a9746929faee09794",
      "4a5cdefd2f3e4b7796fb815ce4e503be",
      "b097c4184ce843c6b7c51bd5a9b6d351",
      "8d331b688cfa4844915d56273c5c5bf8",
      "c0a4e2ac65cc4ec0bc38aba6b6aa0b6f",
      "52eb8c5c4fc24760ba176a0881766513",
      "ee7f39082a59490db5120b42a94171e7",
      "12c1150fb2504c8ab365f1250eaaf6db",
      "b96d1bb024064e12b8b2c679d106ced0",
      "7e0f6e4b454a417b817ebed1c628514e",
      "eb03e4709f81417797d96331c9ac9dd8",
      "52a9311b75934990ace379ef6ccbe11e",
      "30a1f918e08f4a29ae198c1c1dbb94fe",
      "61e1a52c9405457bb467e4cf7ea859eb",
      "aca35bf80ac549fe97420968d8714956",
      "137de352b4314cbe8d66420b2f1bc245",
      "3777ef4f914944cf96f9c55c6ab41098",
      "acbf57cc781f45cfae5ee7a7e92cc1a8",
      "4b977faaed2e4c5084186ed9ab082a48",
      "71acc918d12e444bbe1ddbf02ee66b09",
      "d210dcc40c244cc9b861b84e31f1e089",
      "69bdb6551d9249459663fabf9a46ce43",
      "376f5efdaf7944f181bf654e23b499bb",
      "d5ca7d4b421c4e50abdc9aebb4a9181d",
      "2472a3aa15b048f580e1133777afdb54",
      "19e69fcd9fef49a9a8505bb5d0d4251e",
      "135ab57968ee461cb077f57b5a7d3c94",
      "5f95d3b30b424382acfeed92ed38027a",
      "dbd071578bca44608c31f84e993e1410",
      "9b762681e644425389ca40397cc1135e",
      "e1419ce9d4c54aeba40e19b826175f82",
      "8868571fe43b4e33afbb69779ebc6910",
      "459d025b3ff341dca54b37e06b1d64f4",
      "dffab145e2d74c1bb0b0728d3a1a1a72",
      "21ef433c26af418987224d9cc27c9625",
      "b4a90a0d32534477b4312ba35c7bd22f",
      "732dde25bdbf4a979de890c2a1dac5ab",
      "2ab50bbc82464646beefbeb5741dec8b",
      "890978963be94e2194beb7edda7fa92f",
      "009c65581d6d4b60aabe86e8517debfc",
      "34d39cc7080142f29f3b16c69a4334ba",
      "1e59657107d3478ea505f30122a85a37",
      "b594e605c7964cdf8d83bb29af4c234b",
      "86c44deef0b54df3ab09b756d93304d2",
      "3110573fe4b449b6bfd4ea23a8074d4b",
      "2711c4550ae844b9b063d6c9f64f47f5",
      "bcc9d533b60d4eb28d68c51241d5c4bb",
      "162370a94e7f4c24b204418069a367e6",
      "950dc2c19a334d918cbd1f23d34efbac",
      "17c6a90b79794f40b590bd901bcedb17",
      "17117a6e412f47588ccd02c2b6deca86",
      "c880bcb90a3e4f2ca9ea44cc862f58b0",
      "109716e6a9fa4798a5a6f4b44f2a3784",
      "9a4ecc4fcde44bc690dfd615c3d10628",
      "3f44add0ab7e424791726ed6f35fe65b",
      "4dc24a3c428c47f2bf9f0ffe2980e367",
      "8f8d5b1c700a44809f853043b33d7bfe",
      "7841f611b67f462981e3ca816b6c4f28",
      "67145d29715b47f399035b46cb2fcab6",
      "948002c1522746cab060a561caf122e0",
      "267c9e221b8b40c087d39e3a61fd0540",
      "517752c539304d448f61bbad8c3cd80b",
      "02d2fff15e3e4136af92671c164950de",
      "f8b9b56c810b450a84db8747ae473dbb",
      "32914d6289364d1a9405adeb730ad870",
      "ed34f8d5431f4919a28015965130ea0d",
      "86769a77dd264d829053f09c2e1c05fe",
      "ca827fb5de2043e38f5ed569a96febfc",
      "dd5a0c165cf946dbb9becf2ebc2664f8",
      "67a92162ddca4ace8a705980132492ac",
      "624afae6750645e69398f1f2d44e27c6",
      "5fcac63ffa794a4d91af8430e7808d32",
      "e33cc979c86a4a3bb8a669b4284eac21",
      "bb53667bcff94be7950de5ea30a3af1d",
      "9161176ea6834cac9eb8267284f55058",
      "55aed65366ed4ebaa45dcb9ec49c958a",
      "a83ebe941ed04228a6c74e82b76b4c8c",
      "8e4decb26edc4f88b2fd50572588d0a2",
      "0e8234b8174f497a965ce5fd8b586b03",
      "4a01ad9f83aa48b9a46071843deb75dc",
      "1db3913ca9474db9a756c4002afe4f75",
      "64cd6c4b238f4424acad4462f4df66a8",
      "4482f3b4077441909b134143d461a110",
      "2c047d33847c468bae097cd70d356e7a",
      "82a1c49bf6e64add81d5a31864adadd9",
      "18008075f0fc4177954dcbb7b30875de",
      "fc1c0d8123db4fdea521ccff12e46a61",
      "af0ec58ae24d4b2fb1c8c643273fbb6f",
      "983132048b25416098e5186fa7e2c238",
      "c3fb89889c294f82a46fa5922880a0f2",
      "6fc0436fbc0449aca8193a372c2de74c",
      "c063ad6c80354589b227ade70a73393a",
      "e9ec86186dd84cd0b74750abd29a323c",
      "fd1f814aee5f42c2a2d79a169defc13a",
      "eb5fddd7603a4603aea9b04a1a0f7b39",
      "e775af6faa644cf6b09ddc538b82da69",
      "78c1ad9018a545d5b5f9d3412d1650f4",
      "fe6833ff38a449568b72a7d56416ae78",
      "d8e28e2f6f5e42d6a16c89296821e609",
      "df863716aa1644389705ce4e70548c42",
      "e13d5d72fa1648b4a4fb36fc092e6bc5",
      "278abe4b2d89435ba0b708a8604424f5",
      "3438fbab18be41c0bd4eefad151bd716",
      "90bb5dc008f84ed5b4b6b259a9a7151b",
      "c5bbab705af1495b961dcefb92a811f9",
      "e2416ac1300945d5bc2eecfecbc3dcc7",
      "af9b5ff71819490ab960cedee2c17726",
      "92ea001b75e04932bd393bed4f6103dd",
      "afce36a0df434ef2a410695a212f057f",
      "1dd206f239de4defa82805b2755f3a82",
      "14822ec6802a42788c6246de948723cd"
     ]
    },
    "id": "PH2RqZ9HIBxt",
    "outputId": "c3680ae9-66f1-40c3-f304-80187012c628"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee946d6e375e4b5bbd219184c0338834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5cdefd2f3e4b7796fb815ce4e503be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a1f918e08f4a29ae198c1c1dbb94fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ca7d4b421c4e50abdc9aebb4a9181d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ef433c26af418987224d9cc27c9625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2711c4550ae844b9b063d6c9f64f47f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8d5b1c700a44809f853043b33d7bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca827fb5de2043e38f5ed569a96febfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8234b8174f497a965ce5fd8b586b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fb89889c294f82a46fa5922880a0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13d5d72fa1648b4a4fb36fc092e6bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 384\n",
      "Sample embedding values: [0.06836465001106262, 0.06170124188065529, -0.006391868460923433, 0.08266667276620865, -0.07825048267841339]\n"
     ]
    }
   ],
   "source": [
    "# Use Sentence Transformers for embeddings, with GPU support (updated import to avoid deprecation)\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cuda\"}  # Use L4 GPU\n",
    ")\n",
    "\n",
    "# Test embedding (debug: generate and check shape)\n",
    "test_embedding = embeddings.embed_query(\"Test query\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")  # Should be 384 for this model\n",
    "print(\"Sample embedding values:\", test_embedding[:5])  # First few values for sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyaYwKnQIBvE",
    "outputId": "92f892c1-04a9-4289-876a-65ddc8c3a4cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 docs\n",
      "Sample retrieved content: Log ID: 102, Engine RPM: 1481.0, Lubricant Oil Pressure: 3.972600005 bar, Coolant Pressure: 1.556111313 bar, Coolant Temp: 93.79160853 C, Engine Condition: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-4060366680.py:13: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  test_docs = retriever.get_relevant_documents(\"engine overheating\")\n"
     ]
    }
   ],
   "source": [
    "# Create FAISS index from documents\n",
    "# Split docs into chunks if needed; here they're short\n",
    "from langchain.docstore.document import Document\n",
    "docs = [Document(page_content=doc) for doc in documents]\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Save locally (optional)\n",
    "vectorstore.save_local(\"auto_maintenance_index\")\n",
    "\n",
    "# Test retrieval (debug: query and check results)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "test_docs = retriever.get_relevant_documents(\"engine overheating\")\n",
    "print(f\"Retrieved {len(test_docs)} docs\")\n",
    "print(\"Sample retrieved content:\", test_docs[0].page_content)  # Check relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 640,
     "referenced_widgets": [
      "541e2c158e2441c983be9a0aaed326e6",
      "676d906223d0433cb68d5d743dab1cf6",
      "f9e1a6ecaff74d7a9cb3bace7464e97f",
      "95198807b73e40f9a1f1251c10733dbc",
      "6d9a391f77614ac09422ad8d5b03d955",
      "50df1d67d2b24141b4b980df4d4822df",
      "ced717df4e5042d8819890f1f3eb9095",
      "42a3f036312d4a8cb59cbf82207ebf04",
      "73ba764af8be44d8a65298ba8ed700b6",
      "d0525816db48428fa2df1426067fb987",
      "32879219e3f046ae9a5121cc73a048cd",
      "4b0ae675a81f4370b67527810bd0f20d",
      "cd43e23814aa46c6ae6535b7f52fa583",
      "075635336dfd44499068578fc16ece24",
      "d591dfe4760b41b19f931fb0a03510d6",
      "82de7da7b7734f25b665a2aa14e30452",
      "1117f25d90da4e2099c77d46441258a3",
      "0a358a27b17b4e7e9ca7874f326cf25f",
      "97e27599a0c94e62826f8410146e88cf",
      "3503a01ff29b4ab09c3f6c1befbd31df",
      "ea911b98d82a4a7087799f686b8a2e5b",
      "b37e7da079014cc09436faa3d082f410",
      "df61fac28c904dda9082f8f10fbc6ac7",
      "38789dc8e7b04986bb3167d00af5b77f",
      "56921223d9c545569fda21cd1903ffae",
      "809de698f2e4417080b8c417e6941e3e",
      "220f235f0a004c2fb6f2d78a7f569237",
      "e450e57ca6964b699c090bea93ed1f39",
      "0bcdba836af74b34891dfc0652fec725",
      "016ae2e252404fe8b7f407b730e1f668",
      "5fe049f9db754202b05411ab24c9aa98",
      "294a9d58b0f94112a53c3798485f8b8a",
      "db5c22da96174564987282e76481d8bd",
      "b0c2a98662d34eb38f7997436fcf63c6",
      "9092948711374293bc4f725e978fe094",
      "0a652a7fcc3749e7aa450f5dc7e13fbc",
      "dc3b7be39f15494e9eaf284e75939c2a",
      "e81ef139564145eb9eaed2b992a93b52",
      "c21702f023d743e5984ba54598c3b44f",
      "17449e67d804420d839f7bbce97abfd4",
      "6a89c37d0ebd4207bcc63262fcb06672",
      "64a4d17ce38347af8259468bb4125fb7",
      "8bedb5e9f0fc4e61a6a8d76094bedcd5",
      "fef9b10e613f409b91749c8d704758bf",
      "b7933399671446ffabe81f551a5d7cbd",
      "9adc19592275470b9359f8148775ecb0",
      "3d981ce7b6c543238f910f774fae08a2",
      "e8bfbc58efa54e2b99c6cc1cf7f8ee49",
      "58bd35a6d6344a83904909d9e50948f2",
      "1f5c22e02ee24fc79a4a171b0c83f581",
      "44ef7fb979504999a448b4a09a27f368",
      "645b1d64aa954c909df58b3d05e0f3e6",
      "e0630e9ae76c4a36bf15646320e0c6bf",
      "c3cd2be8aec44771a15c7b95755c1347",
      "0182a9c01f12482ca1c2919392274c0a",
      "f28f8b98c22f4669a7a7fdc3981fb849",
      "34eb81fb4bf248119b79d726098b218b",
      "1402ae28837e49d4b460cdac19370671",
      "193291f2e9df473c93d538603ccf989d",
      "c1ec8d8a6de44fa783dff83cbfbb6f94",
      "f5a381d7cf7b43b6aea866e95dbd2d9e",
      "49f8bfda91d64599a2f6b92de5d56646",
      "de2e459c4d4345a7a7fbd3be0bab2940",
      "04a2700ea75045f796a95eceed0b68fe",
      "afa7ef095d054da284bbc6592f889971",
      "c2bf3d616df749d39b02ad2b359a6554",
      "cc53574502984762b7a0d13e09e07814",
      "b0fb57c1b2e8408cb3e6d89633afa888",
      "ee56d608bf3542308f848b31d25dcb80",
      "7aa978c4ff3c4b5ba401a638f10d9c2f",
      "c0e6a7b41e38441ab6f08430a89a7504",
      "2bdeed35717640d88da37541101fad72",
      "dddd6338eb9741228e2293ca1d53b05c",
      "38ff37f27e9e4300989991a0fc6b75d2",
      "bb972e44a85448aeaa0062e34087a8e1",
      "1e93a206b0f746f89157338cf30794cc",
      "8a0fc9806ad545d388f166ba386f24dd",
      "3224049fa84d494ab41eddf90cfa3c3c",
      "b83fb99c022647b3927fcda91288127f",
      "848afa64582042beb54489b3b9579915",
      "799237bf0c95461dae51e018f5a80c1b",
      "e1f6a493b8e14835944851a24bd21cdb",
      "a8060e3e35054ed7a410cf37003ed4a9",
      "d05fcc83b4a740368cde835c46ec20fa",
      "6bd16a8fc66245a4a0ce1b187fddad1b",
      "8b55906304974a3cbb1b4b8e391368b9",
      "95a581a93b554d9cb028b40cf69d42f5",
      "08f7ea76e73f4c8cb9cf667c157ab4b1",
      "60bf635e93a84f3eb8139b40c6f032d1",
      "7550bdc6f63d442e8615b6f51c6ebb37",
      "af07a72d35fc463e99ea3c9788143279",
      "43e0b83fccb84e3c92ab11f544905346",
      "634ec986c5d647a4b438eb4b51953e78",
      "c277dc85e23f4c5aad597f75d5b41e7c",
      "8931ee238f144ef9a69494aadae3ecf7",
      "a87fd22ab3fd4a7b958f0485877e72f2",
      "dc6992383bc84a36bb17bd613e909659",
      "feaaea23817c4f019b06624ac76a090a",
      "56d4e87c90d146d1aab040ce84ab5013",
      "6dd459100a9f4da099067aeb0d8b2f4b",
      "a12a6b6252c54f079fb07e41a4e708bd",
      "673c500730084ebbba9955a55a5b28bf",
      "cb36ff43b4a44b27bfe6683093d8db51",
      "9166071aed5c4f70bd53cd1c19487dd3",
      "ae8cbdd7f2b24374ac0641cc0f956339",
      "174c38c3b3b9409fbb67f68d2fd4c090",
      "030acfe254b2417b963884c7721e4401",
      "7152ab1cc78449d2b8c278a3d11d69bc",
      "4d297f0341b641b4a5597d478be71a02",
      "699906f646354691be084f215afa6767",
      "a9945b044abc4b019b052e0522719953",
      "3df69c1301584ce9b770673432fc4ea0",
      "ccad7d4e89bb49f589538fd501480780",
      "7116aedb639c47a8be9bba91044269e5",
      "d33569b82e1744bd8f9859f38bc900b2",
      "9ea2e24509cd4faca42e27822cf0942e",
      "73709c939cb9445ea9684b9bca70c166",
      "e152866482a5469b99a60a8f87f51141",
      "a802323f51f544fdb2f117c798c82334",
      "ea02240a880247c293a1183b216bbb19",
      "371bea3df132454cb3cce6a02c8b7060",
      "d9d9dc19e8fc4e7aab2092f495ebb458",
      "97974aa881ee4e6381bf8e32e4f1601c",
      "be7ffe1fa2104052a8b1ce24ad2c6b4b",
      "420416ec23a6429aa7b86d036ca8c2f7",
      "3352c1e71869454290c9182db31b82ad",
      "f6e2b1de37914e85a0fd6d1d34580ef0",
      "83784355f8204d9bad75e634558a7a96",
      "bf0ec6b564f049af9e084adf4ce7adda",
      "581b9aa8551b4a5c91e74bf1a80177e5",
      "0a731fce667146f0ace9d2ef48c0c550",
      "2bd97f9781be495eb06988be22de3be6"
     ]
    },
    "id": "okL4ndw_IBss",
    "outputId": "c443a8f0-9353-4766-ee43-9a7c8fa1bb46"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541e2c158e2441c983be9a0aaed326e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0ae675a81f4370b67527810bd0f20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df61fac28c904dda9082f8f10fbc6ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c2a98662d34eb38f7997436fcf63c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7933399671446ffabe81f551a5d7cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28f8b98c22f4669a7a7fdc3981fb849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc53574502984762b7a0d13e09e07814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3224049fa84d494ab41eddf90cfa3c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bf635e93a84f3eb8139b40c6f032d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd459100a9f4da099067aeb0d8b2f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9945b044abc4b019b052e0522719953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d9dc19e8fc4e7aab2092f495ebb458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipython-input-3703066654.py:29: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n",
      "/tmp/ipython-input-3703066654.py:33: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  test_response = llm(test_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM test response: [INST]Hello, what is predictive maintenance?[/INST] Predictive maintenance is a proactive maintenance strategy that uses data analysis tools and techniques to predict when equipment failure might occur, allowing maintenance to be performed just in time to prevent the failure. Unlike traditional maintenance methods that are scheduled at regular intervals (preventive maintenance) or after a failure has occurred (reactive maintenance), predictive maintenance relies on condition-monitoring tools and techniques to assess the actual condition of the equipment to predict when maintenance should be performed.\n",
      "\n",
      "The goal of predictive maintenance is to perform maintenance at the right time, which can reduce downtimes, improve equipment reliability, and extend the life of the equipment. It involves various techniques such as vibration analysis, thermography, oil analysis, and ultrasonic leak detection, among others.\n",
      "\n",
      "Predictive maintenance is part of a larger concept known as the Internet of Things (IoT), where connected devices can communicate with each other and with centralized systems to provide real-time data. This data can then be analyzed using machine learning and artificial intelligence algorithms to predict potential equipment failures before they happen. This approach helps in optimizing maintenance schedules, reducing costs, and improving safety and productivity.\n",
      "\n",
      "By implementing predictive maintenance, organizations can shift from a reactive maintenance approach to a more strategic and efficient one, ultimately leading to significant cost savings and operational improvements.\n"
     ]
    }
   ],
   "source": [
    "# Load quantized LLM (Phi-3-mini) for generation on L4 GPU\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"cuda:0\",  # Use GPU\n",
    "    attn_implementation=\"eager\"  # For compatibility\n",
    ")\n",
    "\n",
    "# Create pipeline with higher max_new_tokens\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=300,  # Increased for fuller responses\n",
    "    temperature=0.5,\n",
    "    do_sample=True,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Test LLM (debug: generate sample response)\n",
    "test_prompt = \"[INST]Hello, what is predictive maintenance?[/INST]\"  # Added [INST] format\n",
    "test_response = llm(test_prompt)\n",
    "print(\"LLM test response:\", test_response)  # Check if it generates coherent text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxD998z2IBqO",
    "outputId": "749c503c-fa30-426a-d12a-ecf456b336d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined query: Analyze logs for truck with high coolant temp and vibrations. \n"
     ]
    }
   ],
   "source": [
    "# NLP layer: Process input for entities and refinement\n",
    "def process_input(input_data):\n",
    "    doc = nlp(input_data)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    # Simple refinement: Append entities to query for better retrieval\n",
    "    refined = input_data + \" \" + \" \".join([ent[0] for ent in entities])\n",
    "    return refined, entities\n",
    "\n",
    "# Test function (debug: run sample)\n",
    "test_input = \"Analyze logs for truck with high coolant temp and vibrations.\"\n",
    "refined, entities = process_input(test_input)\n",
    "print(\"Refined query:\", refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vduPRsYMIBne",
    "outputId": "5a580359-060c-4e19-bc2c-3dd7e3cb8789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted response: Prediction: High Coolant Temperature at Idle\n",
      "Suggested Actions:\n",
      "1. Check for coolant leaks or insufficient coolant levels.\n",
      "2. Inspect the cooling system for clogs or restrictions in the radiator or hoses.\n",
      "3. Ensure that the cooling fan is operational when the engine is off.\n",
      "4. Verify that the thermostat is functioning correctly and opening at the proper temperature.\n",
      "5. Check for proper airflow through the radiator (e.g., ensure the radiator fan is working, and there are no obstructions).\n",
      "\n",
      "Costs:\n",
      "1. Coolant leak inspection and repair: $100 - $300\n",
      "2. Radiator inspection and cleaning: $50 - $150\n",
      "3. Cooling fan replacement (if necessary): $50 - $100\n",
      "4. Thermostat replacement (if faulty): $10 - $20\n",
      "5. Radiator fan replacement (if necessary): $30 - $60\n",
      "\n",
      "Urgency:\n",
      "1. Coolant leak inspection and repair: Immediate\n",
      "2. Radiator inspection and cleaning: Within 1 week\n",
      "3. Cooling fan replacement: Immediate\n",
      "4. Thermostat replacement: Within 1 week\n",
      "5.\n",
      "Entities: [('XYZ', 'PERSON')]\n",
      "Retrieved docs count: 3\n"
     ]
    }
   ],
   "source": [
    "# Combine NLP, RAG, LLM for prediction\n",
    "def predict_maintenance(input_data):\n",
    "    refined, entities = process_input(input_data)\n",
    "    docs = retriever.get_relevant_documents(refined)\n",
    "\n",
    "    # Format context\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # Updated prompt with specialization and constraints\n",
    "    prompt = f\"[INST]You are an advanced predictive maintenance AI specializing in combustion engines. Use the provided context to predict potential failures and suggest detailed actions. Constraints: Focus only on combustion engines (ignore EVs), include approximate cost estimates for actions (in USD), and assess urgency based on time (e.g., immediate, within 1 week). Output in a structured format: Prediction, Suggested Actions, Costs, Urgency.\\nContext: {context}\\nQuery: {input_data}[/INST]\"\n",
    "\n",
    "    response = llm(prompt)\n",
    "\n",
    "    # Clean response\n",
    "    if \"[INST]\" in response:\n",
    "        response = response.split(\"[/INST]\")[-1].strip()\n",
    "\n",
    "    return response, entities, docs\n",
    "\n",
    "# Test pipeline (debug: full run)\n",
    "test_data = \"High coolant temp in truck XYZ at idle.\"\n",
    "response, entities, docs = predict_maintenance(test_data)\n",
    "print(\"Predicted response:\", response)\n",
    "print(\"Entities:\", entities)\n",
    "print(\"Retrieved docs count:\", len(docs))  # Verify integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605,
     "referenced_widgets": [
      "4289b32960644d1d9ed487f5ce8f62fc",
      "0ea89d6876834cd6ad084e0a6d4175ac",
      "d7e3d911632145e48668b14bfe2d8593",
      "035c641d873548fdb5c60c227b1bcd40",
      "dd005b3aad6a46afbee6e79a6d5c5184",
      "24680ba05fb1483081a1ae88a50a69ec",
      "86f38bb8bf854bdea66a5badbf40211e",
      "761e053995544fb8aa3e8db01ecd7ac9",
      "c46047a810414b4e8a241d59012ba42b",
      "a0f45da2185b418cac8ccdd54ca00ec2",
      "82722066a50e4c379b4a90eb040a730d",
      "ccbd010e247543ba9deadeb4f2d42539",
      "25c1a140efdc4189a68eda4faba81d94",
      "ded17558bb2d4f71b2920197c7d83a5d",
      "e27274de865445749b5aa8c6978ddaee",
      "7afefbc635ba496b882b6accc965fe1b",
      "01d4febd3956469c8b11e921cd7efb43",
      "724b9b254a9446989713d916f88cae2d",
      "02ed7aabf48347ee8c43a9ea5d160646",
      "c20ffe5f5f1f4661bdc4071f1c471fc1",
      "7115bd4f8a9b49fe85a1740fb7558fe2",
      "11fd4bf813e44911a24ea9fac264dd78",
      "e2c4eed7797c44bf830946672ea40ea3",
      "ebb0b7aa3f3a42fb8f83645d1124de06",
      "e8038e7ac71e4e35aa9e23e5850cebb5"
     ]
    },
    "id": "UFKMJzx2gnrv",
    "outputId": "b3366827-fcf2-45f2-dcbc-3177879c4eea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4289b32960644d1d9ed487f5ce8f62fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='text-align:center; color:#000000;'>Predictive Automotive Maintenance Tooâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import ipywidgets for interactive UI\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "\n",
    "# Reuse the existing functions: process_input and predict_maintenance from previous cells\n",
    "# (Assume they've been defined)\n",
    "\n",
    "# Create widgets\n",
    "header = widgets.HTML(value=\"<h2 style='text-align:center; color:#000000;'>Predictive Automotive Maintenance Tool</h2>\")\n",
    "\n",
    "input_text = widgets.Text(\n",
    "    value='High engine temperature when speed is above 100 KM/H and AC also stopped working automatically',\n",
    "    placeholder='Enter vehicle logs or sensor data...',\n",
    "    description='Input:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='50%', margin='10px 0')\n",
    ")\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Predict Maintenance',\n",
    "    disabled=False,\n",
    "    button_style='success',  # Green style\n",
    "    tooltip='Run prediction',\n",
    "    icon='wrench',  # Better icon for maintenance\n",
    "    layout=widgets.Layout(width='200px', margin='10px auto', display='block')\n",
    ")\n",
    "\n",
    "output = widgets.Output(layout=widgets.Layout(width='100%', padding='10px', border='1px solid #ddd', background='#f9f9f9'))\n",
    "\n",
    "# Overall layout using VBox for structured UI\n",
    "ui_layout = widgets.VBox(\n",
    "    [header, input_text, button, output],\n",
    "    layout=widgets.Layout(align_items='center', width='100%')\n",
    ")\n",
    "\n",
    "# Define button click handler\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        clear_output(wait=True)  # Clear previous output\n",
    "\n",
    "        if not input_text.value.strip():\n",
    "            display(Markdown(\"**Error:** Please enter some input data.\"))\n",
    "            return\n",
    "\n",
    "        # Show loading indicator with dots\n",
    "        loading = widgets.Label(value=\"Processing\")\n",
    "        display(loading)\n",
    "\n",
    "        try:\n",
    "            import time\n",
    "            dots = \"\"\n",
    "            for i in range(3):  # Cycle through 3 dots\n",
    "                dots = \".\" * (i + 1)\n",
    "                loading.value = f\"Processing{dots}\"\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            response, entities, docs = predict_maintenance(input_text.value)\n",
    "\n",
    "            # Filter response to remove Urgency section\n",
    "            if \"Urgency:\" in response:\n",
    "                response = response.split(\"Urgency:\")[0].strip()\n",
    "\n",
    "            # Ensure costs have $ symbol by simple string replacement if missing\n",
    "            if \"Costs:\" in response and \"$\" not in response:\n",
    "                response = response.replace(\"Costs:\", \"Costs (in $):\")\n",
    "            elif \"Costs:\" in response:\n",
    "                # Basic fix for incomplete $ in example like \"$200âˆ’\" but assume LLM handles\n",
    "                pass\n",
    "\n",
    "            clear_output(wait=True)  # Clear loading\n",
    "\n",
    "            # Display only the main prediction response (Prediction, Suggested Actions, Costs)\n",
    "            display(Markdown(\"**Maintenance Recommendations:**\\n\\n\" + response))\n",
    "\n",
    "        except Exception as e:\n",
    "            clear_output(wait=True)\n",
    "            display(Markdown(f\"**Error during prediction:** {str(e)}\"))\n",
    "\n",
    "# Attach handler to button\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Display the enhanced UI\n",
    "display(ui_layout)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
